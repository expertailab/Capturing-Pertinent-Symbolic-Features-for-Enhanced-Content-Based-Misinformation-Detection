{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoAdapterModel, \n",
    "    AutoTokenizer, \n",
    "    PfeifferConfig,\n",
    "    TrainingArguments, \n",
    "    AdapterTrainer,\n",
    "    AutoConfig, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# Constants\n",
    "\n",
    "DATA_PATH = \"../data/processed/\"\n",
    "MODELS_PATH = \"../models/fewshot/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train task adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"task_name\": \"twittercovidq2\",\n",
    "    \"model_name\": \"roberta-large\",\n",
    "    \"max_length\": 128,\n",
    "    \"batch_size\": 1,\n",
    "    \"epochs\": 30,\n",
    "    \"seeds\" : [0],\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"fewshot_train\": [10, 25, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PATH = f'{DATA_PATH}{CONFIG[\"task_name\"]}.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.read_csv(TASK_PATH).dropna()\n",
    "task_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'no_false', 1: 'contains_false'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {}\n",
    "pos_labels = [\"contains-bias\", \"clickbait\", \"false\", \"fake\", \"has_propaganda\", \"yes\", \"contains_false\"]\n",
    "\n",
    "labels = set(task_df[\"labels\"].to_list())\n",
    "for label in labels:\n",
    "    if str(label).lower() in pos_labels:\n",
    "        id2label.update({1: label})\n",
    "    else:\n",
    "        id2label.update({0: label})\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>The country is panic stricken over the #corona...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Epidemiologist Marc Lipsitch, director of Harv...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>â ï¸Doctors in #Italy warn Europe to âget ...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Government of India issues advisory to all soc...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Our #FlattenTheCurve graphic is now up on @Wik...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>#FakeNews Alert  #PIBFactCheck: The claim that...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>EXCLUSIVE Nadine Dorries, a health minister, h...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>WTAF?!?! A Man at Dartmouth with symptoms of #...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Things the GOP has done during the Covid-19 ou...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Covid-19 hit during cold/flu season. Wondering...</td>\n",
       "      <td>no_false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    labels\n",
       "233  The country is panic stricken over the #corona...  no_false\n",
       "106  Epidemiologist Marc Lipsitch, director of Harv...  no_false\n",
       "237  â ï¸Doctors in #Italy warn Europe to âget ...  no_false\n",
       "76   Government of India issues advisory to all soc...  no_false\n",
       "173  Our #FlattenTheCurve graphic is now up on @Wik...  no_false\n",
       "..                                                 ...       ...\n",
       "67   #FakeNews Alert  #PIBFactCheck: The claim that...  no_false\n",
       "192  EXCLUSIVE Nadine Dorries, a health minister, h...  no_false\n",
       "117  WTAF?!?! A Man at Dartmouth with symptoms of #...  no_false\n",
       "47   Things the GOP has done during the Covid-19 ou...  no_false\n",
       "172  Covid-19 hit during cold/flu season. Wondering...  no_false\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df.sample(frac=1, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation = True\n",
    "padding = \"max_length\"\n",
    "batched = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=CONFIG[\"max_length\"], truncation=truncation, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c27dc7402a46268ce356bcb742ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f654990e14aca80676099596c86eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_dataset = Dataset.from_pandas(task_df)\n",
    "# Encode the input data\n",
    "task_dataset = task_dataset.map(encode_batch, batched=batched)\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "task_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "task_dataset = task_dataset.class_encode_column(\"labels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterDropTrainerCallback(TrainerCallback):\n",
    "  def on_step_begin(self, args, state, control, **kwargs):\n",
    "    skip_layers = list(range(np.random.randint(0, 11)))\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=skip_layers)\n",
    "\n",
    "  def on_evaluate(self, args, state, control, **kwargs):\n",
    "    # Deactivate skipping layers during evaluation (otherwise it would use the\n",
    "    # previous randomly chosen skip_layers and thus yield results not comparable\n",
    "    # across different epochs)\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_and_f1(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = float(f1_score(y_true, y_pred, average='macro'))\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return acc_and_f1(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"epoch\"\n",
    "output_dir = f'{MODELS_PATH}{CONFIG[\"model_name\"]}{os.sep}van-adapt{os.sep}{CONFIG[\"task_name\"]}'\n",
    "overwrite_output_dir = True\n",
    "remove_unused_columns = False\n",
    "save_total_limit = 1\n",
    "report_to = \"wandb\"\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model = \"eval_f1\"\n",
    "early_stopping_patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    task_model = AutoAdapterModel.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        config=config\n",
    "    )\n",
    "    adapter_config = PfeifferConfig()\n",
    "    task_model.add_adapter(CONFIG[\"task_name\"], config=adapter_config)\n",
    "    task_model.train_adapter(CONFIG[\"task_name\"])\n",
    "    task_model.add_classification_head(\n",
    "        CONFIG[\"task_name\"],\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    task_model.set_active_adapters(CONFIG[\"task_name\"])\n",
    "    \n",
    "    return task_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflaviomerenda\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flavio.merenda@EXPERT.AI/atd/notebooks/wandb/run-20230725_134153-4g021915</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/4g021915' target=\"_blank\">seed_0</a></strong> to <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/4g021915' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/4g021915</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "/opt/anaconda3/envs/atd/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 4223490\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/300 00:27 < 00:48, 3.95 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.500416</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>0.643016</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.715440</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.538372</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.610805</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.503131</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.518570</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>0.578355</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.688301</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.640391</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.601019</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-2] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20/twittercovidq2/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10 (score: 0.4791666666666667).\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/adapter_config.json\n",
      "Overwriting existing adapter 'twittercovidq2'.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/pytorch_adapter.bin\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/head_config.json\n",
      "Overwriting existing head 'twittercovidq2'\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'contains_false': 1, 'no_false': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba943b0b12f541329565a09dae6284e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f1</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▁▆█▂▅▁▂▄▇▆▄</td></tr><tr><td>eval/runtime</td><td>▁▁▁▃▁▂████▇</td></tr><tr><td>eval/samples_per_second</td><td>███▅▇▆▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>███▅▇▆▁▁▁▁▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>▂▁▄▂▃▂▂▂▁█▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.92</td></tr><tr><td>eval/f1</td><td>0.47917</td></tr><tr><td>eval/loss</td><td>0.60102</td></tr><tr><td>eval/runtime</td><td>2.018</td></tr><tr><td>eval/samples_per_second</td><td>24.777</td></tr><tr><td>eval/steps_per_second</td><td>24.777</td></tr><tr><td>test/accuracy</td><td>0.84</td></tr><tr><td>test/f1</td><td>0.45652</td></tr><tr><td>test/loss</td><td>0.80115</td></tr><tr><td>test/runtime</td><td>7.9224</td></tr><tr><td>test/samples_per_second</td><td>25.245</td></tr><tr><td>test/steps_per_second</td><td>25.245</td></tr><tr><td>train/epoch</td><td>11.0</td></tr><tr><td>train/global_step</td><td>110</td></tr><tr><td>train/learning_rate</td><td>6e-05</td></tr><tr><td>train/loss</td><td>0.3929</td></tr><tr><td>train/total_flos</td><td>25984739742720.0</td></tr><tr><td>train/train_loss</td><td>0.48554</td></tr><tr><td>train/train_runtime</td><td>27.6491</td></tr><tr><td>train/train_samples_per_second</td><td>10.85</td></tr><tr><td>train/train_steps_per_second</td><td>10.85</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">seed_0</strong> at: <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/4g021915' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/4g021915</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230725_134153-4g021915/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0129b3d0d9bc447ca19be0f351dd031f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666863966577997, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flavio.merenda@EXPERT.AI/atd/notebooks/wandb/run-20230725_134249-q4quomhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/q4quomhq' target=\"_blank\">seed_0</a></strong> to <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/q4quomhq' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/q4quomhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "/opt/anaconda3/envs/atd/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 26\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 780\n",
      "  Number of trainable parameters = 4223490\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/780 00:40 < 01:09, 7.07 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.119200</td>\n",
       "      <td>0.642922</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.217700</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.121600</td>\n",
       "      <td>0.680189</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.942300</td>\n",
       "      <td>0.605946</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.548486</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.545502</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.581208</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.893100</td>\n",
       "      <td>0.547561</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.538219</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.559078</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.533090</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-78] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-234] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26 (score: 0.47191011235955055).\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/adapter_config.json\n",
      "Overwriting existing adapter 'twittercovidq2'.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/pytorch_adapter.bin\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/head_config.json\n",
      "Overwriting existing head 'twittercovidq2'\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'contains_false': 1, 'no_false': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-286] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 187\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='187' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [187/187 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c95c4e24c46898c23aaf811c6fae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f1</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▆▄█▄▂▂▃▂▁▂▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▂▇▇▇▇▇█▇</td></tr><tr><td>eval/samples_per_second</td><td>▇██▆▁▂▂▂▂▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▇██▆▁▂▂▂▂▁▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>▇█▇▅▅▃▅▄▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89362</td></tr><tr><td>eval/f1</td><td>0.47191</td></tr><tr><td>eval/loss</td><td>0.53309</td></tr><tr><td>eval/runtime</td><td>1.7785</td></tr><tr><td>eval/samples_per_second</td><td>26.427</td></tr><tr><td>eval/steps_per_second</td><td>26.427</td></tr><tr><td>test/accuracy</td><td>0.85027</td></tr><tr><td>test/f1</td><td>0.45954</td></tr><tr><td>test/loss</td><td>1.09017</td></tr><tr><td>test/runtime</td><td>7.5546</td></tr><tr><td>test/samples_per_second</td><td>24.753</td></tr><tr><td>test/steps_per_second</td><td>24.753</td></tr><tr><td>train/epoch</td><td>11.0</td></tr><tr><td>train/global_step</td><td>286</td></tr><tr><td>train/learning_rate</td><td>6e-05</td></tr><tr><td>train/loss</td><td>0.6757</td></tr><tr><td>train/total_flos</td><td>67560323331072.0</td></tr><tr><td>train/train_loss</td><td>0.90264</td></tr><tr><td>train/train_runtime</td><td>40.2589</td></tr><tr><td>train/train_samples_per_second</td><td>19.375</td></tr><tr><td>train/train_steps_per_second</td><td>19.375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">seed_0</strong> at: <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/q4quomhq' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/q4quomhq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230725_134249-q4quomhq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9058d5880d23462685218faab598e373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669048085653535, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flavio.merenda@EXPERT.AI/atd/notebooks/wandb/run-20230725_134355-hn2tency</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/hn2tency' target=\"_blank\">seed_0</a></strong> to <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/flaviomerenda/twittercovidq2' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/hn2tency' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/hn2tency</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_false\",\n",
      "    \"1\": \"contains_false\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'twittercovidq2'.\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no_false': 0, 'contains_false': 1}, 'use_pooler': False, 'bias': True}.\n",
      "/opt/anaconda3/envs/atd/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 52\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1560\n",
      "  Number of trainable parameters = 4223490\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='572' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 572/1560 01:03 < 01:49, 9.04 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.628107</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.550608</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.843100</td>\n",
       "      <td>0.591061</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.593347</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.792200</td>\n",
       "      <td>0.602059</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.487319</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.428833</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.593300</td>\n",
       "      <td>0.622719</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.603600</td>\n",
       "      <td>0.939314</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>0.809589</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.820468</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-26] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104/twittercovidq2/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-156] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-208] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-312] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-364] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-468] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/adapter_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/pytorch_adapter.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/pytorch_model_head.bin\n",
      "Configuration saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/head_config.json\n",
      "Module weights saved in ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-520] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52 (score: 0.47500000000000003).\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/adapter_config.json\n",
      "Overwriting existing adapter 'twittercovidq2'.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_adapter.bin\n",
      "Loading module configuration from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/head_config.json\n",
      "Overwriting existing head 'twittercovidq2'\n",
      "Adding head 'twittercovidq2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'contains_false': 1, 'no_false': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-52/twittercovidq2/pytorch_model_head.bin\n",
      "Deleting older checkpoint [../models/fewshot/roberta-large/van-adapt/twittercovidq2/checkpoint-572] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 166\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166/166 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/f1</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▄▃▃▃▃▂▁▄█▆▆</td></tr><tr><td>eval/runtime</td><td>▁▁▂▇█▇▇▇▇▇▇</td></tr><tr><td>eval/samples_per_second</td><td>██▆▁▁▁▁▁▁▁▂</td></tr><tr><td>eval/steps_per_second</td><td>██▆▁▁▁▁▁▁▁▂</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>██▇█▇▆▅▅▅▃▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.90476</td></tr><tr><td>eval/f1</td><td>0.475</td></tr><tr><td>eval/loss</td><td>0.82047</td></tr><tr><td>eval/runtime</td><td>1.697</td></tr><tr><td>eval/samples_per_second</td><td>24.75</td></tr><tr><td>eval/steps_per_second</td><td>24.75</td></tr><tr><td>test/accuracy</td><td>0.8494</td></tr><tr><td>test/f1</td><td>0.45928</td></tr><tr><td>test/loss</td><td>0.95559</td></tr><tr><td>test/runtime</td><td>6.5421</td></tr><tr><td>test/samples_per_second</td><td>25.374</td></tr><tr><td>test/steps_per_second</td><td>25.374</td></tr><tr><td>train/epoch</td><td>11.0</td></tr><tr><td>train/global_step</td><td>572</td></tr><tr><td>train/learning_rate</td><td>6e-05</td></tr><tr><td>train/loss</td><td>0.2008</td></tr><tr><td>train/total_flos</td><td>135120646662144.0</td></tr><tr><td>train/train_loss</td><td>0.6664</td></tr><tr><td>train/train_runtime</td><td>63.0969</td></tr><tr><td>train/train_samples_per_second</td><td>24.724</td></tr><tr><td>train/train_steps_per_second</td><td>24.724</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">seed_0</strong> at: <a href='https://wandb.ai/flaviomerenda/twittercovidq2/runs/hn2tency' target=\"_blank\">https://wandb.ai/flaviomerenda/twittercovidq2/runs/hn2tency</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230725_134355-hn2tency/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fs in CONFIG[\"fewshot_train\"]:\n",
    "    fewshot_train_ratio = np.ceil(fs/len(task_df)*100)\n",
    "    for seed in CONFIG[\"seeds\"]:\n",
    "        wandb.init(\n",
    "            project=CONFIG[\"task_name\"], \n",
    "            config=CONFIG,\n",
    "            job_type=f'{CONFIG[\"model_name\"]}_{fs}',\n",
    "            group=\"van_head\",\n",
    "            tags=[\n",
    "                \"van_head\",\n",
    "                CONFIG['model_name'],\n",
    "                f\"mx: {CONFIG['max_length']}\",\n",
    "                f\"bs: {CONFIG['batch_size']}\",\n",
    "                f\"ep: {CONFIG['epochs']}\",\n",
    "                f\"lr: {CONFIG['learning_rate']}\"\n",
    "            ],\n",
    "            name=f'seed_{seed}',\n",
    "            anonymous='must'\n",
    "        )\n",
    "\n",
    "        train_test = task_dataset.train_test_split(test_size=(100-fewshot_train_ratio)/100, generator=np.random.RandomState(0))\n",
    "        test_valid = train_test['test'].train_test_split(test_size=0.2, generator=np.random.RandomState(0))\n",
    "        \n",
    "        dataset = DatasetDict(\n",
    "            {\n",
    "                'train': train_test['train'],\n",
    "                'valid': test_valid['test'],\n",
    "                'test': test_valid['train']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=CONFIG[\"learning_rate\"],\n",
    "            num_train_epochs=CONFIG[\"epochs\"],\n",
    "            per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "            per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "            gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "            logging_strategy=strategy,\n",
    "            evaluation_strategy=strategy,\n",
    "            save_strategy=strategy,\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=overwrite_output_dir,\n",
    "            # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "            remove_unused_columns=remove_unused_columns,\n",
    "            save_total_limit=save_total_limit,\n",
    "            report_to=report_to,\n",
    "            load_best_model_at_end=load_best_model_at_end,\n",
    "            metric_for_best_model=metric_for_best_model,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model_init=get_model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"valid\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks = [\n",
    "                EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
    "                AdapterDropTrainerCallback()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.evaluate(dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
