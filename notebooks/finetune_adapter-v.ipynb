{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoAdapterModel, \n",
    "    AutoTokenizer, \n",
    "    PfeifferConfig,\n",
    "    TrainingArguments, \n",
    "    AdapterTrainer,\n",
    "    AutoConfig, \n",
    "    TrainerCallback, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# Constants\n",
    "\n",
    "DATA_PATH = \"../data/processed/\"\n",
    "MODELS_PATH = \"../models/finetuning/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train task adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"task_name\": \"clickbait\",\n",
    "    \"model_name\": \"roberta-large\",\n",
    "    \"max_length\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"seeds\" : [0, 21, 48],\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gradient_accumulation_steps\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PATH = f'{DATA_PATH}{CONFIG[\"task_name\"]}.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7959, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.read_csv(TASK_PATH)\n",
    "task_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'no-bias', 1: 'contains-bias'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {}\n",
    "pos_labels = [\"contains-bias\", \"clickbait\", \"false\", \"fake\"]\n",
    "\n",
    "labels = set(task_df[\"labels\"].to_list())\n",
    "for label in labels:\n",
    "    if str(label).lower() in pos_labels:\n",
    "        id2label.update({1: label})\n",
    "    else:\n",
    "        id2label.update({0: label})\n",
    "\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation = True\n",
    "padding = \"max_length\"\n",
    "batched = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=CONFIG[\"max_length\"], truncation=truncation, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d5189cae3648f198dcca605cd519c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8057a3740daf4c77839a4d66c509ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/7959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_dataset = Dataset.from_pandas(task_df)\n",
    "# Encode the input data\n",
    "task_dataset = task_dataset.map(encode_batch, batched=batched)\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "task_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "task_dataset = task_dataset.class_encode_column(\"labels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterDropTrainerCallback(TrainerCallback):\n",
    "  def on_step_begin(self, args, state, control, **kwargs):\n",
    "    skip_layers = list(range(np.random.randint(0, 11)))\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=skip_layers)\n",
    "\n",
    "  def on_evaluate(self, args, state, control, **kwargs):\n",
    "    # Deactivate skipping layers during evaluation (otherwise it would use the\n",
    "    # previous randomly chosen skip_layers and thus yield results not comparable\n",
    "    # across different epochs)\n",
    "    kwargs['model'].set_active_adapters(kwargs['model'].active_adapters[0], skip_layers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_and_f1(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = float(f1_score(y_true, y_pred, average='macro'))\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return acc_and_f1(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"epoch\"\n",
    "output_dir = f'{MODELS_PATH}{CONFIG[\"model_name\"]}{os.sep}van-adapt{os.sep}{CONFIG[\"task_name\"]}'\n",
    "overwrite_output_dir = True\n",
    "remove_unused_columns = False\n",
    "save_total_limit = 1\n",
    "report_to = \"wandb\"\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model = \"eval_f1\"\n",
    "early_stopping_patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    task_model = AutoAdapterModel.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        config=config\n",
    "    )\n",
    "    adapter_config = PfeifferConfig()\n",
    "    task_model.add_adapter(CONFIG[\"task_name\"], config=adapter_config)\n",
    "    task_model.train_adapter(CONFIG[\"task_name\"])\n",
    "    task_model.add_classification_head(\n",
    "        CONFIG[\"task_name\"],\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "    )\n",
    "    task_model.set_active_adapters(CONFIG[\"task_name\"])\n",
    "    \n",
    "    return task_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflaviomerenda\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/flavio.merenda@EXPERT.AI/atd/notebooks/wandb/run-20230714_174718-2ylsqj5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flaviomerenda/basil/runs/2ylsqj5b' target=\"_blank\">seed_0</a></strong> to <a href='https://wandb.ai/flaviomerenda/basil' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/flaviomerenda/basil' target=\"_blank\">https://wandb.ai/flaviomerenda/basil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/flaviomerenda/basil/runs/2ylsqj5b' target=\"_blank\">https://wandb.ai/flaviomerenda/basil/runs/2ylsqj5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no-bias\",\n",
      "    \"1\": \"contains-bias\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaAdapterModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Adding adapter 'basil'.\n",
      "Adding head 'basil' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'no-bias': 0, 'contains-bias': 1}, 'use_pooler': False, 'bias': True}.\n",
      "loading configuration file config.json from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no-bias\",\n",
      "    \"1\": \"contains-bias\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/flavio.merenda@EXPERT.AI/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in CONFIG[\"seeds\"]:\n",
    "    wandb.init(\n",
    "        project=CONFIG[\"task_name\"], \n",
    "        config=CONFIG,\n",
    "        job_type=CONFIG['model_name'],\n",
    "        group=\"van_head\",\n",
    "        tags=[\n",
    "            \"van_head\",\n",
    "            CONFIG['model_name'],\n",
    "            f\"mx: {CONFIG['max_length']}\",\n",
    "            f\"bs: {CONFIG['batch_size']}\",\n",
    "            f\"ep: {CONFIG['epochs']}\",\n",
    "            f\"lr: {CONFIG['learning_rate']}\"\n",
    "        ],\n",
    "        name=f'seed_{seed}',\n",
    "        anonymous='must'\n",
    "    )\n",
    "\n",
    "    train_test = task_dataset.train_test_split(test_size=0.1, generator=np.random.RandomState(0))\n",
    "    train_valid = train_test['train'].train_test_split(test_size=0.15, generator=np.random.RandomState(0))\n",
    "    \n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            'train': train_valid['train'],\n",
    "            'valid': train_valid['test'],\n",
    "            'test': train_test['test']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        num_train_epochs=CONFIG[\"epochs\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        logging_strategy=strategy,\n",
    "        evaluation_strategy=strategy,\n",
    "        save_strategy=strategy,\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "        remove_unused_columns=remove_unused_columns,\n",
    "        save_total_limit=save_total_limit,\n",
    "        report_to=report_to,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    trainer = AdapterTrainer(\n",
    "        model_init=get_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"valid\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [\n",
    "            EarlyStoppingCallback(early_stopping_patience=early_stopping_patience),\n",
    "            AdapterDropTrainerCallback()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate(dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
